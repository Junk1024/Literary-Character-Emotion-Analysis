{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99942ba5-68a3-4ff9-9c0f-aaa0b91046f5",
   "metadata": {},
   "source": [
    "# 一、项目背景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25365a52-9697-483f-b647-89bd00c18c22",
   "metadata": {},
   "source": [
    "本项目主要基于PaddleNLP通过预训练模型NeZha在SMP2020微博情绪6分类数据集上进行微调从而完成6分类情感分析模型的搭建，使用前后端分离的方式搭建Web端交互平台，支持文本细粒度情感分类预测，具有前沿性和广泛的应用价值。\n",
    "\n",
    "情感分析在当下信息产业时代具有重要作用：在舆情分析方面，通过对热点事件进行情感剖析，寻找情感原因，对政府了解民意，预防危害事件的发生具有一定的意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9e804-bfff-413f-a85d-d2441e173f7f",
   "metadata": {},
   "source": [
    "# 二、项目方案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85721884-1c69-46b2-8987-81becdc3eec5",
   "metadata": {},
   "source": [
    "## 总技术路线\n",
    "\n",
    "基于PaddleNLP通过预训练模型NeZha在SMP2020微博情绪6分类数据集上进行微调从而完成6分类情感分析模型的搭建，自动识别文本中的情绪信息；\n",
    "\n",
    "使用 Vue 和 FastAPI 实现 Web 前后端分离部署，更方便地实现对自然文本中地情感识别。\n",
    "\n",
    "![技术路线](https://ai-studio-static-online.cdn.bcebos.com/855b6274ef794f57a4fe861b7438cdc96f63231b5cca4a839b361ae7f0cb2661)\n",
    "\n",
    "## 运行环境要求\n",
    "\n",
    "注意模型训练需要使用GPU环境\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ddb5c9-e809-4895-97f5-93edad7631ff",
   "metadata": {},
   "source": [
    "# 三、数据说明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d16cb2-ee2f-4f92-87be-11421c95b0b3",
   "metadata": {},
   "source": [
    "数据来源：SMP2020微博情绪分类技术评测\n",
    "\n",
    "本次使用数据集为SMP2020微博情绪分类技术评测数据集（SMP2020-EWECT）\n",
    "\n",
    "该技术评测使用的标注数据集由哈尔滨工业大学社会计算与信息检索研究中心提供，原始数据源于新浪微博，由微热点大数据研究院提供，数据集分为两部分。\n",
    "\n",
    "第一部分为通用微博数据集，该数据集内的微博内容是随机获取到微博内容，不针对特定的话题，覆盖的范围较广。\n",
    "\n",
    "第二部分为疫情微博数据集，该数据集内的微博内容是在疫情期间使用相关关键字筛选获得的疫情微博，其内容与新冠疫情相关。\n",
    "\n",
    "每条微博被标注为以下六个类别之一：neutral（无情绪）、happy（积极）、angry（愤怒）、sad（悲伤）、fear（恐惧）、surprise（惊奇）。\n",
    "\n",
    "通用微博训练数据集包括27,768条微博，验证集包含2,000条微博，测试数据集包含5,000条微博。\n",
    "\n",
    "疫情微博训练数据集包括8,606条微博，验证集包含2,000条微博，测试数据集包含3,000条微博。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682ab75-f537-4eff-aa91-f47854034740",
   "metadata": {},
   "source": [
    "## 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f370385f-748b-43cf-9cc4-039c85a96582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:02:59.999054Z",
     "iopub.status.busy": "2022-12-07T16:02:59.998585Z",
     "iopub.status.idle": "2022-12-07T16:03:00.450739Z",
     "shell.execute_reply": "2022-12-07T16:03:00.449826Z",
     "shell.execute_reply.started": "2022-12-07T16:02:59.999023Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\r\n",
      "Int64Index: 48374 entries, 0 to 2999\r\n",
      "Data columns (total 3 columns):\r\n",
      " #   Column  Non-Null Count  Dtype \r\n",
      "---  ------  --------------  ----- \r\n",
      " 0   数据编号    48374 non-null  int64 \r\n",
      " 1   文本      48372 non-null  object\r\n",
      " 2   情绪标签    48374 non-null  object\r\n",
      "dtypes: int64(1), object(2)\r\n",
      "memory usage: 1.5+ MB\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>数据编号</th>\n",
       "      <th>文本</th>\n",
       "      <th>情绪标签</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>气死姐姐了，快二是阵亡了吗，尼玛，一个半小时过去了也没上车</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>妞妞啊，今天又承办了一个发文登记文号是126~嘻~么么哒~晚安哟</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>这里还值得注意另一个事实，就是张鞠存原有一个东溪草堂为其读书处。</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>这在前华约国家(尤其是东德)使用R-73的首次联合演习期间，被一些北约组织的飞行员所证实。</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>TinyThief上wii了？！</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   数据编号                                             文本      情绪标签\n",
       "0     1                  气死姐姐了，快二是阵亡了吗，尼玛，一个半小时过去了也没上车     angry\n",
       "1     2               妞妞啊，今天又承办了一个发文登记文号是126~嘻~么么哒~晚安哟     happy\n",
       "2     3               这里还值得注意另一个事实，就是张鞠存原有一个东溪草堂为其读书处。   neutral\n",
       "3     4  这在前华约国家(尤其是东德)使用R-73的首次联合演习期间，被一些北约组织的飞行员所证实。   neutral\n",
       "4     5                               TinyThief上wii了？！  surprise"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过pandas读取并处理数据\n",
    "import pandas as pd\n",
    "\n",
    "# 训练数据集\n",
    "train1 = pd.read_csv('./usual_train.csv')\n",
    "train2 = pd.read_csv('./virus_train.csv')\n",
    "\n",
    "# 验证数据集\n",
    "dev1 = pd.read_csv('./usual_eval_labeled.csv')\n",
    "dev2 = pd.read_csv('./virus_eval_labeled.csv')\n",
    "\n",
    "# 测试数据集\n",
    "test1 = pd.read_csv('./usual_test_labeled.csv')\n",
    "test2 = pd.read_csv('./virus_test_labeled.csv')\n",
    "\n",
    "# 合并数据集\n",
    "train = pd.concat([train1, train2])\n",
    "dev = pd.concat([dev1, dev2])\n",
    "test = pd.concat([test1, test2])\n",
    "\n",
    "# 构造总数据集便于统计分析\n",
    "total = pd.concat([train, dev, test])\n",
    "\n",
    "total.info()\n",
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53b1494-5221-4804-8f19-3fea72058083",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:03:04.299292Z",
     "iopub.status.busy": "2022-12-07T16:03:04.298845Z",
     "iopub.status.idle": "2022-12-07T16:03:04.335911Z",
     "shell.execute_reply": "2022-12-07T16:03:04.335196Z",
     "shell.execute_reply.started": "2022-12-07T16:03:04.299263Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>气死姐姐了，快二是阵亡了吗，尼玛，一个半小时过去了也没上车</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>妞妞啊，今天又承办了一个发文登记文号是126~嘻~么么哒~晚安哟</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>这里还值得注意另一个事实，就是张鞠存原有一个东溪草堂为其读书处。</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>这在前华约国家(尤其是东德)使用R-73的首次联合演习期间，被一些北约组织的飞行员所证实。</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TinyThief上wii了？！</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_a     label\n",
       "0                  气死姐姐了，快二是阵亡了吗，尼玛，一个半小时过去了也没上车     angry\n",
       "1               妞妞啊，今天又承办了一个发文登记文号是126~嘻~么么哒~晚安哟     happy\n",
       "2               这里还值得注意另一个事实，就是张鞠存原有一个东溪草堂为其读书处。   neutral\n",
       "3  这在前华约国家(尤其是东德)使用R-73的首次联合演习期间，被一些北约组织的飞行员所证实。   neutral\n",
       "4                               TinyThief上wii了？！  surprise"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将数据处理为text_a, label的格式便于进行统一处理\n",
    "train['text_a'] = train['文本']\n",
    "dev['text_a'] = dev['文本']\n",
    "test['text_a'] = test['文本']\n",
    "total['text_a'] = total['文本']\n",
    "\n",
    "train['label'] = train['情绪标签']\n",
    "dev['label'] = dev['情绪标签']\n",
    "test['label'] = test['情绪标签']\n",
    "total['label'] = total['情绪标签']\n",
    "\n",
    "train = train[['text_a', 'label']]\n",
    "dev = dev[['text_a', 'label']]\n",
    "test = test[['text_a', 'label']]\n",
    "total = total[['text_a', 'label']]\n",
    "\n",
    "total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c586bcc-3d61-4334-9078-54b14c421595",
   "metadata": {},
   "source": [
    "## 数据集检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f017c833-d365-4d3c-bad5-bbb920aff21f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:03:07.667578Z",
     "iopub.status.busy": "2022-12-07T16:03:07.667155Z",
     "iopub.status.idle": "2022-12-07T16:03:07.686766Z",
     "shell.execute_reply": "2022-12-07T16:03:07.686204Z",
     "shell.execute_reply.started": "2022-12-07T16:03:07.667551Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\r\n",
      "Int64Index: 48374 entries, 0 to 2999\r\n",
      "Data columns (total 2 columns):\r\n",
      " #   Column  Non-Null Count  Dtype \r\n",
      "---  ------  --------------  ----- \r\n",
      " 0   text_a  48372 non-null  object\r\n",
      " 1   label   48374 non-null  object\r\n",
      "dtypes: object(2)\r\n",
      "memory usage: 1.1+ MB\r\n",
      "<class 'pandas.core.frame.DataFrame'>\r\n",
      "Int64Index: 36374 entries, 0 to 8605\r\n",
      "Data columns (total 2 columns):\r\n",
      " #   Column  Non-Null Count  Dtype \r\n",
      "---  ------  --------------  ----- \r\n",
      " 0   text_a  36372 non-null  object\r\n",
      " 1   label   36374 non-null  object\r\n",
      "dtypes: object(2)\r\n",
      "memory usage: 852.5+ KB\r\n"
     ]
    }
   ],
   "source": [
    "# 查看数据文件信息\n",
    "total.info()\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78acee09-6594-47f1-bb3e-e97b0fba22cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:03:19.896909Z",
     "iopub.status.busy": "2022-12-07T16:03:19.896394Z",
     "iopub.status.idle": "2022-12-07T16:03:19.914951Z",
     "shell.execute_reply": "2022-12-07T16:03:19.914279Z",
     "shell.execute_reply.started": "2022-12-07T16:03:19.896876Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 可以发现训练数据中text_a存在缺失，直接清除缺失行\n",
    "train = train.dropna(subset = ['text_a'])\n",
    "total = total.dropna(subset = ['text_a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e0274bd-216a-4b1b-8480-116f1ff05672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:03:22.564323Z",
     "iopub.status.busy": "2022-12-07T16:03:22.563880Z",
     "iopub.status.idle": "2022-12-07T16:03:22.573847Z",
     "shell.execute_reply": "2022-12-07T16:03:22.573318Z",
     "shell.execute_reply.started": "2022-12-07T16:03:22.564295Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "happy       13673\n",
       "angry       12536\n",
       "neutral      9615\n",
       "sad          7269\n",
       "surprise     2942\n",
       "fear         2337\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计标签分布情况\n",
    "total['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1cfa2c-7ac7-433a-845a-27233dad1072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:03:25.612299Z",
     "iopub.status.busy": "2022-12-07T16:03:25.611877Z",
     "iopub.status.idle": "2022-12-07T16:03:25.720198Z",
     "shell.execute_reply": "2022-12-07T16:03:25.719577Z",
     "shell.execute_reply.started": "2022-12-07T16:03:25.612270Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 对处理后的数据进行存储\n",
    "train.to_csv('train.csv', sep='\\t', index = False)\n",
    "dev.to_csv('dev.csv', sep='\\t', index = False)\n",
    "test.to_csv('test.csv', sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e79cbc-42c0-4342-a1c8-e725c794e5c8",
   "metadata": {},
   "source": [
    "# 四、基于PaddleNLP构建微情感分析模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab147fd7-3cad-45df-a8c4-f0d6182fa5c1",
   "metadata": {},
   "source": [
    "PaddleNLP 是飞桨自然语言处理开发库，具备 易用的文本领域API，多场景的应用示例、和 高性能分布式训练 三大特点，旨在提升飞桨开发者文本领域建模效率，旨在提升开发者在文本领域的开发效率，并提供丰富的NLP应用示例。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3802c796-85f6-4997-9e68-5d8f24141127",
   "metadata": {},
   "source": [
    "## 4.1 加载 NeZha 预训练模型\n",
    "\n",
    "NEZHA是华为在预训练模型上的实践总结，它在BERT的基础上加了很多当下有用的优化，比如Functional Relative Positional Encoding、Whole Word Masking策略、混合精度训练和Lamb优化器。实验表明，NEZHA在多项具有代表性的NLU任务上均取得了不错的成绩。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/d295f95afd504f32897d2ad17c85cc3c1ee97b5b9f3a41b5b3e6ee2a142b0a66)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cace4f57-433e-457f-89ab-5ce82aa26f6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:03:30.797862Z",
     "iopub.status.busy": "2022-12-07T16:03:30.797409Z",
     "iopub.status.idle": "2022-12-07T16:03:32.183849Z",
     "shell.execute_reply": "2022-12-07T16:03:32.182684Z",
     "shell.execute_reply.started": "2022-12-07T16:03:30.797831Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入所需的第三方库\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "from functools import partial\n",
    "import random\n",
    "import time\n",
    "import inspect\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import IterableDataset\n",
    "from paddle.utils.download import get_path_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c074358-2a7e-48ce-b469-cdf43119602b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:03:34.511585Z",
     "iopub.status.busy": "2022-12-07T16:03:34.511180Z",
     "iopub.status.idle": "2022-12-07T16:03:44.926622Z",
     "shell.execute_reply": "2022-12-07T16:03:44.925634Z",
     "shell.execute_reply.started": "2022-12-07T16:03:34.511557Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\n",
      "Requirement already satisfied: paddlenlp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.4.2)\r\n",
      "Collecting paddlenlp\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d4/6e/209c5b64d45ef0dfea06da9a597dab09b27293fd9d6da9c1064e50de5030/paddlenlp-2.4.4-py3-none-any.whl (2.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting uvicorn\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/96/f3/f39ac8ac3bdf356b4934b8f7e56173e96681f67ef0cd92bd33a5059fae9e/uvicorn-0.20.0-py3-none-any.whl (56 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.64.1)\r\n",
      "Requirement already satisfied: protobuf<=3.20.0,>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (3.20.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.11.0)\r\n",
      "Requirement already satisfied: paddle2onnx in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.0.0)\r\n",
      "Collecting typer\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0d/44/56c3f48d2bb83d76f5c970aef8e2c3ebd6a832f09e3621c5395371fe6999/typer-0.7.0-py3-none-any.whl (38 kB)\r\n",
      "Requirement already satisfied: rich in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (12.6.0)\r\n",
      "Requirement already satisfied: dill<0.3.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.3.3)\r\n",
      "Requirement already satisfied: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.4.0)\r\n",
      "Requirement already satisfied: multiprocess<=0.70.12.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\r\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.7.0)\r\n",
      "Requirement already satisfied: paddlefsl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.1.0)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.1.96)\r\n",
      "Requirement already satisfied: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\r\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\r\n",
      "Collecting fastapi\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d8/09/ce090f6d53ce8b6335954488087210fa1e054c4a65f74d5f76aed254c159/fastapi-0.88.0-py3-none-any.whl (55 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (4.2.0)\r\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (0.18.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (5.1.2)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (2022.11.0)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (2.24.0)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (3.8.3)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (21.3)\r\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (10.0.0)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (3.1.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (1.1.5)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (1.19.5)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub>=0.10.1->paddlenlp) (4.3.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub>=0.10.1->paddlenlp) (3.0.12)\r\n",
      "Collecting starlette==0.22.0\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1d/4e/30eda84159d5b3ad7fe663c40c49b16dd17436abe838f10a56c34bee44e8/starlette-0.22.0-py3-none-any.whl (64 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/22/53/196c9a5752e30d682e493d7c00ea0a02377446578e577ae5e085010dc0bd/pydantic-1.10.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from starlette==0.22.0->fastapi->paddlenlp) (3.6.1)\r\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rich->paddlenlp) (0.9.1)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rich->paddlenlp) (2.13.0)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from typer->paddlenlp) (8.0.4)\r\n",
      "Collecting h11>=0.8\r\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl (58 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.2.3)\r\n",
      "Requirement already satisfied: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (8.2.0)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.16.0)\r\n",
      "Requirement already satisfied: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\r\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\r\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\r\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (3.0.0)\r\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\r\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\r\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (2.1.1)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (1.3.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (6.0.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (22.1.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (1.2.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (1.7.2)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (0.13.0)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (4.0.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging->datasets>=2.0.0->paddlenlp) (3.0.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (2019.9.11)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (1.25.11)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (2.8)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\r\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\r\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata->datasets>=2.0.0->paddlenlp) (3.8.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (2.8.2)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (1.1.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (0.10.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi->paddlenlp) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp) (2.0.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->visualdl->paddlenlp) (56.2.0)\r\n",
      "Installing collected packages: pydantic, h11, starlette, uvicorn, typer, fastapi, paddlenlp\r\n",
      "  Attempting uninstall: paddlenlp\r\n",
      "    Found existing installation: paddlenlp 2.4.2\r\n",
      "    Uninstalling paddlenlp-2.4.2:\r\n",
      "      Successfully uninstalled paddlenlp-2.4.2\r\n",
      "Successfully installed fastapi-0.88.0 h11-0.14.0 paddlenlp-2.4.4 pydantic-1.10.2 starlette-0.22.0 typer-0.7.0 uvicorn-0.20.0\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# 安装最新的paddlenlp\n",
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90c5cb82-14db-4e28-8218-a59400d5543d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:03:55.300417Z",
     "iopub.status.busy": "2022-12-07T16:03:55.299943Z",
     "iopub.status.idle": "2022-12-07T16:03:56.360022Z",
     "shell.execute_reply": "2022-12-07T16:03:56.359100Z",
     "shell.execute_reply.started": "2022-12-07T16:03:55.300391Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 导入paddlenlp相关的包\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddle.dataset.common import md5file\n",
    "from paddlenlp.datasets import DatasetBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a4f1a-8535-4196-939f-c4386a3cd3b5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 调用华为的NeZha模型，同时因为本任务的情感分类是6类，设置num_classes = 6\n",
    "model = ppnlp.transformers.NeZhaForSequenceClassification.from_pretrained('nezha-large-wwm-chinese', num_classes = 6)\n",
    "tokenizer = ppnlp.transformers.NeZhaTokenizer.from_pretrained('nezha-large-wwm-chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a42f98e-3037-4255-a8c5-f81510f0e795",
   "metadata": {},
   "source": [
    "## 4.2 模型训练前置工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3cb2abf-f517-46da-9ef9-2c0bbc3c3e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:05:06.549632Z",
     "iopub.status.busy": "2022-12-07T16:05:06.549166Z",
     "iopub.status.idle": "2022-12-07T16:05:06.557440Z",
     "shell.execute_reply": "2022-12-07T16:05:06.556885Z",
     "shell.execute_reply.started": "2022-12-07T16:05:06.549604Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义数据集对应文件及其文件存储格式\n",
    "class EmotionData(DatasetBuilder):\n",
    "    SPLITS = {\n",
    "        'train': 'train.csv',  # 训练集\n",
    "        'dev': 'dev.csv',    # 验证集\n",
    "        'test': 'test.csv',    # 测试集\n",
    "    }\n",
    "\n",
    "    def _get_data(self, mode, **kwargs):\n",
    "        filename = self.SPLITS[mode]\n",
    "        return filename\n",
    "\n",
    "    def _read(self, filename):\n",
    "        \"\"\"读取数据\"\"\"\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            head = None\n",
    "            for line in f:\n",
    "                data = line.strip().split(\"\\t\")    # 以'\\t'分隔各列\n",
    "                if not head:\n",
    "                    head = data\n",
    "                else:\n",
    "                    text_a, label = data\n",
    "                    yield {\"text_a\": text_a, \"label\": label}  # 数据的格式：text_a,label\n",
    "\n",
    "    def get_labels(self):\n",
    "        return label_list   # 类别标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84f7182d-cbf9-4343-b1ce-efcb478835c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:05:10.364642Z",
     "iopub.status.busy": "2022-12-07T16:05:10.364184Z",
     "iopub.status.idle": "2022-12-07T16:05:10.370643Z",
     "shell.execute_reply": "2022-12-07T16:05:10.369964Z",
     "shell.execute_reply.started": "2022-12-07T16:05:10.364610Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义数据集对应文件及其文件存储格式\n",
    "class EmotionData(DatasetBuilder):\n",
    "    SPLITS = {\n",
    "        'train': 'train.csv',  # 训练集\n",
    "        'dev': 'dev.csv',    # 验证集\n",
    "        'test': 'test.csv',    # 测试集\n",
    "    }\n",
    "\n",
    "    def _get_data(self, mode, **kwargs):\n",
    "        filename = self.SPLITS[mode]\n",
    "        return filename\n",
    "\n",
    "    def _read(self, filename):\n",
    "        \"\"\"读取数据\"\"\"\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            head = None\n",
    "            for line in f:\n",
    "                data = line.strip().split(\"\\t\")    # 以'\\t'分隔各列\n",
    "                if not head:\n",
    "                    head = data\n",
    "                else:\n",
    "                    text_a, label = data\n",
    "                    yield {\"text_a\": text_a, \"label\": label}  # 数据的格式：text_a,label\n",
    "\n",
    "    def get_labels(self):\n",
    "        return label_list   # 类别标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d007c4d-40e6-4be7-8c5f-e9c5ab96a075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:07:17.126276Z",
     "iopub.status.busy": "2022-12-07T16:07:17.125869Z",
     "iopub.status.idle": "2022-12-07T16:07:17.130658Z",
     "shell.execute_reply": "2022-12-07T16:07:17.130110Z",
     "shell.execute_reply.started": "2022-12-07T16:07:17.126246Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义数据集加载函数\n",
    "def load_dataset(name=None,\n",
    "                 data_files=None,\n",
    "                 splits=None,\n",
    "                 lazy=None,\n",
    "                 **kwargs):\n",
    "   \n",
    "    reader_cls = EmotionData\n",
    "    print(reader_cls)\n",
    "    if not name:\n",
    "        reader_instance = reader_cls(lazy=lazy, **kwargs)\n",
    "    else:\n",
    "        reader_instance = reader_cls(lazy=lazy, name=name, **kwargs)\n",
    "\n",
    "    datasets = reader_instance.read_datasets(data_files=data_files, splits=splits)\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60d89f6f-7c7b-46ee-93fc-7673b38e0a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:07:20.335105Z",
     "iopub.status.busy": "2022-12-07T16:07:20.334724Z",
     "iopub.status.idle": "2022-12-07T16:07:20.341358Z",
     "shell.execute_reply": "2022-12-07T16:07:20.340793Z",
     "shell.execute_reply.started": "2022-12-07T16:07:20.335079Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义数据加载和处理函数\n",
    "def convert_example(example, tokenizer, max_seq_length=512, is_test=False):\n",
    "    qtconcat = example[\"text_a\"]\n",
    "    encoded_inputs = tokenizer(text=qtconcat, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "# 数据加载函数dataloader\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab2a8c9-c2fc-49ec-844a-e02e0214234b",
   "metadata": {},
   "source": [
    "## 4.3 配置预训练参数\n",
    "\n",
    "可以通过对部分参数的简单修改，满足相关的训练要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0c2bd95-4639-437e-9a23-c1ef1dbc58a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:08:00.530430Z",
     "iopub.status.busy": "2022-12-07T16:08:00.529957Z",
     "iopub.status.idle": "2022-12-07T16:08:00.535724Z",
     "shell.execute_reply": "2022-12-07T16:08:00.535167Z",
     "shell.execute_reply.started": "2022-12-07T16:08:00.530396Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'angry', 1: 'happy', 2: 'neutral', 3: 'surprise', 4: 'sad', 5: 'fear'}\r\n"
     ]
    }
   ],
   "source": [
    "# 定义要进行分类的类别\n",
    "label_list = ['angry', 'happy', 'neutral', 'surprise', 'sad', 'fear']\n",
    "label_map = {idx: label for idx, label in enumerate(label_list)}\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cf0dd6a-c5f2-4393-b7b6-07d5f3aac729",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:08:02.546517Z",
     "iopub.status.busy": "2022-12-07T16:08:02.546057Z",
     "iopub.status.idle": "2022-12-07T16:08:02.646282Z",
     "shell.execute_reply": "2022-12-07T16:08:02.645537Z",
     "shell.execute_reply.started": "2022-12-07T16:08:02.546487Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.EmotionData'>\r\n"
     ]
    }
   ],
   "source": [
    "# 加载训练集、验证集和测试集\n",
    "train_ds, dev_ds, test_ds = load_dataset(splits=[\"train\", \"dev\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f73b4258-3485-4fa8-aa44-bc0cf2f6699c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:08:18.992539Z",
     "iopub.status.busy": "2022-12-07T16:08:18.992059Z",
     "iopub.status.idle": "2022-12-07T16:08:19.003734Z",
     "shell.execute_reply": "2022-12-07T16:08:19.002940Z",
     "shell.execute_reply.started": "2022-12-07T16:08:18.992509Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 96  #批处理大小，可根据训练环境条件，适当修改此项\n",
    "max_seq_length = 128  #文本序列截断长度\n",
    "\n",
    "# 将数据处理成模型可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "# 训练集迭代器\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "# 验证集迭代器\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "# 测试集迭代器\n",
    "test_data_loader = create_dataloader(\n",
    "    test_ds, \n",
    "    mode='test', \n",
    "    batch_size=batch_size, \n",
    "    batchify_fn=batchify_fn, \n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fea69e24-bd17-4eab-8a1a-48a1a471a213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:08:24.673144Z",
     "iopub.status.busy": "2022-12-07T16:08:24.672648Z",
     "iopub.status.idle": "2022-12-07T16:08:24.681355Z",
     "shell.execute_reply": "2022-12-07T16:08:24.680765Z",
     "shell.execute_reply.started": "2022-12-07T16:08:24.673113Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义超参，loss，优化器等\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "# 定义训练过程中的最大学习率\n",
    "learning_rate = 2e-5\n",
    "# 训练轮次\n",
    "epochs = 3\n",
    "# 学习率预热比例\n",
    "warmup_proportion = 0.1\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "\n",
    "# AdamW优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()  # 交叉熵损失函数\n",
    "metric = paddle.metric.Accuracy()  # accuracy评价指标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f659029d-5634-44a1-b085-577c8c573cd3",
   "metadata": {},
   "source": [
    "## 4.4 训练模型与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b6d1f44-ae41-4edf-a881-d250d3bddb84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:08:28.220197Z",
     "iopub.status.busy": "2022-12-07T16:08:28.219712Z",
     "iopub.status.idle": "2022-12-07T16:08:28.226177Z",
     "shell.execute_reply": "2022-12-07T16:08:28.225591Z",
     "shell.execute_reply.started": "2022-12-07T16:08:28.220164Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义模型训练验证评估函数\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return accu  # 返回准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e5e5bc6-e7bf-49f9-82cd-68c8a0d8fbb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:08:30.733006Z",
     "iopub.status.busy": "2022-12-07T16:08:30.732546Z",
     "iopub.status.idle": "2022-12-07T16:21:04.538300Z",
     "shell.execute_reply": "2022-12-07T16:21:04.537323Z",
     "shell.execute_reply.started": "2022-12-07T16:08:30.732976Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 1.75473, acc: 0.22604\r\n",
      "global step 20, epoch: 1, batch: 20, loss: 1.70342, acc: 0.24375\r\n",
      "global step 30, epoch: 1, batch: 30, loss: 1.69516, acc: 0.24306\r\n",
      "global step 40, epoch: 1, batch: 40, loss: 1.71487, acc: 0.25729\r\n",
      "global step 50, epoch: 1, batch: 50, loss: 1.62575, acc: 0.26813\r\n",
      "global step 60, epoch: 1, batch: 60, loss: 1.63088, acc: 0.28993\r\n",
      "global step 70, epoch: 1, batch: 70, loss: 1.46991, acc: 0.31920\r\n",
      "global step 80, epoch: 1, batch: 80, loss: 1.45474, acc: 0.34622\r\n",
      "global step 90, epoch: 1, batch: 90, loss: 1.32177, acc: 0.37326\r\n",
      "global step 100, epoch: 1, batch: 100, loss: 1.33373, acc: 0.39823\r\n",
      "global step 110, epoch: 1, batch: 110, loss: 1.10039, acc: 0.42055\r\n",
      "global step 120, epoch: 1, batch: 120, loss: 1.06426, acc: 0.43724\r\n",
      "global step 130, epoch: 1, batch: 130, loss: 1.13974, acc: 0.45256\r\n",
      "global step 140, epoch: 1, batch: 140, loss: 1.07292, acc: 0.46659\r\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.96340, acc: 0.48000\r\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.97556, acc: 0.49154\r\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.89148, acc: 0.50368\r\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.86817, acc: 0.51383\r\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.90194, acc: 0.52538\r\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.87227, acc: 0.53443\r\n",
      "global step 210, epoch: 1, batch: 210, loss: 0.72549, acc: 0.54459\r\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.76002, acc: 0.55355\r\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.52115, acc: 0.56377\r\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.56240, acc: 0.57261\r\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.63353, acc: 0.58025\r\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.49265, acc: 0.58814\r\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.65739, acc: 0.59518\r\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.62537, acc: 0.60108\r\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.75954, acc: 0.60593\r\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.66963, acc: 0.61149\r\n",
      "global step 310, epoch: 1, batch: 310, loss: 0.66411, acc: 0.61677\r\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.76251, acc: 0.62139\r\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.69762, acc: 0.62579\r\n",
      "global step 340, epoch: 1, batch: 340, loss: 0.79137, acc: 0.63067\r\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.53416, acc: 0.63476\r\n",
      "global step 360, epoch: 1, batch: 360, loss: 0.61418, acc: 0.63845\r\n",
      "global step 370, epoch: 1, batch: 370, loss: 0.55152, acc: 0.64243\r\n",
      "eval loss: 0.63209, accu: 0.78775\r\n",
      "0.78775\r\n",
      "global step 380, epoch: 2, batch: 1, loss: 0.53746, acc: 0.80208\r\n",
      "global step 390, epoch: 2, batch: 11, loss: 0.65286, acc: 0.81061\r\n",
      "global step 400, epoch: 2, batch: 21, loss: 0.50917, acc: 0.81250\r\n",
      "global step 410, epoch: 2, batch: 31, loss: 0.37266, acc: 0.82224\r\n",
      "global step 420, epoch: 2, batch: 41, loss: 0.55853, acc: 0.82698\r\n",
      "global step 430, epoch: 2, batch: 51, loss: 0.58719, acc: 0.82904\r\n",
      "global step 440, epoch: 2, batch: 61, loss: 0.49778, acc: 0.82548\r\n",
      "global step 450, epoch: 2, batch: 71, loss: 0.37378, acc: 0.82673\r\n",
      "global step 460, epoch: 2, batch: 81, loss: 0.50361, acc: 0.82472\r\n",
      "global step 470, epoch: 2, batch: 91, loss: 0.69523, acc: 0.82498\r\n",
      "global step 480, epoch: 2, batch: 101, loss: 0.61036, acc: 0.82508\r\n",
      "global step 490, epoch: 2, batch: 111, loss: 0.43145, acc: 0.82667\r\n",
      "global step 500, epoch: 2, batch: 121, loss: 0.40376, acc: 0.82739\r\n",
      "global step 510, epoch: 2, batch: 131, loss: 0.39617, acc: 0.82888\r\n",
      "global step 520, epoch: 2, batch: 141, loss: 0.54050, acc: 0.82809\r\n",
      "global step 530, epoch: 2, batch: 151, loss: 0.61683, acc: 0.82871\r\n",
      "global step 540, epoch: 2, batch: 161, loss: 0.42943, acc: 0.82971\r\n",
      "global step 550, epoch: 2, batch: 171, loss: 0.64608, acc: 0.82950\r\n",
      "global step 560, epoch: 2, batch: 181, loss: 0.47272, acc: 0.83074\r\n",
      "global step 570, epoch: 2, batch: 191, loss: 0.52347, acc: 0.83082\r\n",
      "global step 580, epoch: 2, batch: 201, loss: 0.47093, acc: 0.83100\r\n",
      "global step 590, epoch: 2, batch: 211, loss: 0.42924, acc: 0.83082\r\n",
      "global step 600, epoch: 2, batch: 221, loss: 0.40764, acc: 0.83065\r\n",
      "global step 610, epoch: 2, batch: 231, loss: 0.58005, acc: 0.82977\r\n",
      "global step 620, epoch: 2, batch: 241, loss: 0.36994, acc: 0.82949\r\n",
      "global step 630, epoch: 2, batch: 251, loss: 0.34169, acc: 0.82922\r\n",
      "global step 640, epoch: 2, batch: 261, loss: 0.36936, acc: 0.83006\r\n",
      "global step 650, epoch: 2, batch: 271, loss: 0.47683, acc: 0.82984\r\n",
      "global step 660, epoch: 2, batch: 281, loss: 0.44185, acc: 0.82989\r\n",
      "global step 670, epoch: 2, batch: 291, loss: 0.46684, acc: 0.83022\r\n",
      "global step 680, epoch: 2, batch: 301, loss: 0.40265, acc: 0.83015\r\n",
      "global step 690, epoch: 2, batch: 311, loss: 0.64986, acc: 0.83055\r\n",
      "global step 700, epoch: 2, batch: 321, loss: 0.68134, acc: 0.83083\r\n",
      "global step 710, epoch: 2, batch: 331, loss: 0.63680, acc: 0.83066\r\n",
      "global step 720, epoch: 2, batch: 341, loss: 0.49144, acc: 0.83049\r\n",
      "global step 730, epoch: 2, batch: 351, loss: 0.39067, acc: 0.83063\r\n",
      "global step 740, epoch: 2, batch: 361, loss: 0.55238, acc: 0.83033\r\n",
      "global step 750, epoch: 2, batch: 371, loss: 0.40664, acc: 0.83016\r\n",
      "eval loss: 0.61294, accu: 0.78925\r\n",
      "0.78925\r\n",
      "global step 760, epoch: 3, batch: 2, loss: 0.34328, acc: 0.88021\r\n",
      "global step 770, epoch: 3, batch: 12, loss: 0.40171, acc: 0.88715\r\n",
      "global step 780, epoch: 3, batch: 22, loss: 0.40608, acc: 0.88305\r\n",
      "global step 790, epoch: 3, batch: 32, loss: 0.44845, acc: 0.88249\r\n",
      "global step 800, epoch: 3, batch: 42, loss: 0.37898, acc: 0.88170\r\n",
      "global step 810, epoch: 3, batch: 52, loss: 0.36286, acc: 0.87941\r\n",
      "global step 820, epoch: 3, batch: 62, loss: 0.26772, acc: 0.88306\r\n",
      "global step 830, epoch: 3, batch: 72, loss: 0.28987, acc: 0.88455\r\n",
      "global step 840, epoch: 3, batch: 82, loss: 0.33990, acc: 0.88796\r\n",
      "global step 850, epoch: 3, batch: 92, loss: 0.21893, acc: 0.88791\r\n",
      "global step 860, epoch: 3, batch: 102, loss: 0.18611, acc: 0.88725\r\n",
      "global step 870, epoch: 3, batch: 112, loss: 0.33433, acc: 0.88746\r\n",
      "global step 880, epoch: 3, batch: 122, loss: 0.28484, acc: 0.88661\r\n",
      "global step 890, epoch: 3, batch: 132, loss: 0.54056, acc: 0.88534\r\n",
      "global step 900, epoch: 3, batch: 142, loss: 0.37709, acc: 0.88534\r\n",
      "global step 910, epoch: 3, batch: 152, loss: 0.40929, acc: 0.88720\r\n",
      "global step 920, epoch: 3, batch: 162, loss: 0.43302, acc: 0.88683\r\n",
      "global step 930, epoch: 3, batch: 172, loss: 0.40455, acc: 0.88796\r\n",
      "global step 940, epoch: 3, batch: 182, loss: 0.39955, acc: 0.88805\r\n",
      "global step 950, epoch: 3, batch: 192, loss: 0.36269, acc: 0.88726\r\n",
      "global step 960, epoch: 3, batch: 202, loss: 0.22641, acc: 0.88769\r\n",
      "global step 970, epoch: 3, batch: 212, loss: 0.26691, acc: 0.88807\r\n",
      "global step 980, epoch: 3, batch: 222, loss: 0.43982, acc: 0.88880\r\n",
      "global step 990, epoch: 3, batch: 232, loss: 0.36726, acc: 0.88842\r\n",
      "global step 1000, epoch: 3, batch: 242, loss: 0.40065, acc: 0.88826\r\n",
      "global step 1010, epoch: 3, batch: 252, loss: 0.35219, acc: 0.88860\r\n",
      "global step 1020, epoch: 3, batch: 262, loss: 0.43374, acc: 0.88884\r\n",
      "global step 1030, epoch: 3, batch: 272, loss: 0.40966, acc: 0.88875\r\n",
      "global step 1040, epoch: 3, batch: 282, loss: 0.27637, acc: 0.88837\r\n",
      "global step 1050, epoch: 3, batch: 292, loss: 0.44369, acc: 0.88831\r\n",
      "global step 1060, epoch: 3, batch: 302, loss: 0.34310, acc: 0.88776\r\n",
      "global step 1070, epoch: 3, batch: 312, loss: 0.35466, acc: 0.88799\r\n",
      "global step 1080, epoch: 3, batch: 322, loss: 0.26116, acc: 0.88823\r\n",
      "global step 1090, epoch: 3, batch: 332, loss: 0.33197, acc: 0.88802\r\n",
      "global step 1100, epoch: 3, batch: 342, loss: 0.33566, acc: 0.88861\r\n",
      "global step 1110, epoch: 3, batch: 352, loss: 0.47299, acc: 0.88867\r\n",
      "global step 1120, epoch: 3, batch: 362, loss: 0.34540, acc: 0.88861\r\n",
      "global step 1130, epoch: 3, batch: 372, loss: 0.50472, acc: 0.88886\r\n",
      "eval loss: 0.63792, accu: 0.79375\r\n",
      "0.79375\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-08 00:21:04,525] [    INFO] - tokenizer config file saved in checkpoint/tokenizer_config.json\r\n",
      "[2022-12-08 00:21:04,528] [    INFO] - Special tokens file saved in checkpoint/special_tokens_map.json\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('checkpoint/tokenizer_config.json',\n",
       " 'checkpoint/special_tokens_map.json',\n",
       " 'checkpoint/added_tokens.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练：\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "save_dir = \"checkpoint\"\n",
    "if not  os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "pre_accu=0\n",
    "accu=0\n",
    "global_step = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "    # 每轮结束对验证集进行评估\n",
    "    accu = evaluate(model, criterion, metric, dev_data_loader)\n",
    "    print(accu)\n",
    "    if accu > pre_accu:\n",
    "        # 保存较上一轮效果更优的模型参数\n",
    "        save_param_path = os.path.join(save_dir, 'model_state.pdparams')  # 保存模型参数\n",
    "        paddle.save(model.state_dict(), save_param_path)\n",
    "        pre_accu=accu\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043da3f2-b55f-47fd-adfd-e9d7e0783d53",
   "metadata": {},
   "source": [
    "## 4.5 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8b3608b-126e-4c63-af16-5d82b1fc3f14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:25:49.577726Z",
     "iopub.status.busy": "2022-12-07T16:25:49.577303Z",
     "iopub.status.idle": "2022-12-07T16:25:49.581557Z",
     "shell.execute_reply": "2022-12-07T16:25:49.580984Z",
     "shell.execute_reply.started": "2022-12-07T16:25:49.577696Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义6个分类类别\n",
    "label_list = ['angry', 'happy', 'neutral', 'surprise', 'sad', 'fear']\n",
    "label_map = {idx: label for idx, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd2ce044-eb75-4395-b4af-dd7115375e47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:27:56.581276Z",
     "iopub.status.busy": "2022-12-07T16:27:56.580846Z",
     "iopub.status.idle": "2022-12-07T16:27:56.589395Z",
     "shell.execute_reply": "2022-12-07T16:27:56.588766Z",
     "shell.execute_reply.started": "2022-12-07T16:27:56.581244Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义模型预测函数\n",
    "def predict(model, data, tokenizer, label_map, batch_size=1):\n",
    "    examples = []\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=128,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # Seperates data into some batches.\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        # The last batch whose size is less than the config batch_size setting.\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac52cec-8b56-4bbe-a865-0f46b0767bbf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入NeZha模型\n",
    "model = ppnlp.transformers.NeZhaForSequenceClassification.from_pretrained('nezha-large-wwm-chinese', num_classes=6)\n",
    "tokenizer = ppnlp.transformers.NeZhaTokenizer.from_pretrained('nezha-large-wwm-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd8e9efb-8689-410e-aaad-ecce9aaa9014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:28:09.010675Z",
     "iopub.status.busy": "2022-12-07T16:28:09.010280Z",
     "iopub.status.idle": "2022-12-07T16:28:10.472628Z",
     "shell.execute_reply": "2022-12-07T16:28:10.471984Z",
     "shell.execute_reply.started": "2022-12-07T16:28:09.010649Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已导入模型参数： checkpoint/model_state.pdparams\r\n"
     ]
    }
   ],
   "source": [
    "# 导入模型权重参数\n",
    "params_path = 'checkpoint/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"已导入模型参数：\", params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "712f9746-216b-496b-8865-8bca86cf59a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:28:13.290150Z",
     "iopub.status.busy": "2022-12-07T16:28:13.289760Z",
     "iopub.status.idle": "2022-12-07T16:28:13.294107Z",
     "shell.execute_reply": "2022-12-07T16:28:13.293506Z",
     "shell.execute_reply.started": "2022-12-07T16:28:13.290121Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义需要预测的语句\n",
    "data = [\n",
    "    # angry\n",
    "    {\"text_a\": '更年期的女boss真的让人受不了，烦躁'},\n",
    "    # fear\n",
    "    {\"text_a\": '尼玛吓死我了，人家剪个头发回来跟劳改犯一样短的可怕，后面什么鬼[黑线][黑线][黑线][白眼][白眼]'},\n",
    "    # neutral\n",
    "    {\"text_a\": \"这个村的年轻人大多数都出外打工。\"},\n",
    "    # surprise\n",
    "    {\"text_a\": \"我竟然才知道我有一个富二代加官二代加红二代的朋友\"},\n",
    "    # sad\n",
    "    {\"text_a\": \"江泽民同志逝世的消息让他十分心痛\"},\n",
    "    # happy\n",
    "    {\"text_a\": \"今天吃火锅，香死我了！！！\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45bfa58a-79de-4aa3-b04c-1b98e087a299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:28:15.937980Z",
     "iopub.status.busy": "2022-12-07T16:28:15.937634Z",
     "iopub.status.idle": "2022-12-07T16:28:16.180660Z",
     "shell.execute_reply": "2022-12-07T16:28:16.179987Z",
     "shell.execute_reply.started": "2022-12-07T16:28:15.937956Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语句: 更年期的女boss真的让人受不了，烦躁 \t 情绪: angry\r\n",
      "语句: 尼玛吓死我了，人家剪个头发回来跟劳改犯一样短的可怕，后面什么鬼[黑线][黑线][黑线][白眼][白眼] \t 情绪: fear\r\n",
      "语句: 这个村的年轻人大多数都出外打工。 \t 情绪: neutral\r\n",
      "语句: 我竟然才知道我有一个富二代加官二代加红二代的朋友 \t 情绪: surprise\r\n",
      "语句: 江泽民同志逝世的消息让他十分心痛 \t 情绪: sad\r\n",
      "语句: 今天吃火锅，香死我了！！！ \t 情绪: happy\r\n"
     ]
    }
   ],
   "source": [
    "# 模型预测结果\n",
    "results = predict(model, data, tokenizer, label_map, batch_size=1)\n",
    "for idx, text in enumerate(data):\n",
    "    print('语句: {} \\t 情绪: {}'.format(text['text_a'], results[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68e367-fdef-4618-8cb0-fd0aab12b986",
   "metadata": {},
   "source": [
    "# 五、基于FastAPI和Vue实现Web可视化开发\n",
    "\n",
    "**web部分需保存好\"checkpoint/\"内训练好地模型参数，本地部署使用。**\n",
    "\n",
    "\n",
    "使用FastAPI需要使用pip工具(```pip install fastapi```)安装好相关依赖，详情可见[FastAPI文档](https://fastapi.tiangolo.com/zh/#_3)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3959ac9c-bd72-4b02-93cc-db93109291f5",
   "metadata": {},
   "source": [
    "## 5.1 后端模型处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54330c02-93fe-41f7-a4db-d1807b004c55",
   "metadata": {},
   "source": [
    "```Python\n",
    "# 导入所需的库\n",
    "import paddle\n",
    "import numpy as np\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Pad, Tuple\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f6cce-d566-4e17-afd1-7cd41b3cb733",
   "metadata": {},
   "source": [
    "```python\n",
    "# 格式化函数\n",
    "def format_print(results, data):\n",
    "    for idx, text in enumerate(data):\n",
    "        print('语句: {} \\t 情绪: {}'.format(text['text_a'], results[idx]))\n",
    "\n",
    "def parseTodata(input_text):\n",
    "    return [{\"text_a\": input_text}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723d28a-4de0-4b20-8865-3704f98eedf0",
   "metadata": {},
   "source": [
    "```python\n",
    "# 定义数据加载和处理函数\n",
    "def convert_example(example, tokenizer, max_seq_length=512, is_test=False):\n",
    "    qtconcat = example[\"text_a\"]\n",
    "    encoded_inputs = tokenizer(text=qtconcat, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "# 定义模型预测函数\n",
    "def predict(model, input_text, tokenizer, label_map, batch_size):\n",
    "    data = parseTodata(input_text)\n",
    "    examples = []\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=128,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # Seperates data into some batches.\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        # The last batch whose size is less than the config batch_size setting.\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    format_print(results, data)\n",
    "    return  results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f5633-6ef9-43c3-8b1f-8b10b06ad579",
   "metadata": {},
   "source": [
    "## 5.2 加载模型，启动后端服务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcadbcf-5fa9-47b8-a4a1-d9fed10f047a",
   "metadata": {},
   "source": [
    "```python\n",
    "# 导入所需的库\n",
    "import os\n",
    "import paddle\n",
    "import paddlenlp as ppnlp\n",
    "from fastapi import FastAPI, HTTPException, UploadFile\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2efbf-c001-45f7-922a-160da85aca40",
   "metadata": {},
   "source": [
    "```python\n",
    "# 定义6个分类类别\n",
    "label_list = ['angry', 'happy', 'neutral', 'surprise', 'sad', 'fear']\n",
    "label_map = {idx: label for idx, label in enumerate(label_list)}\n",
    "\n",
    "# 导入NeZha模型\n",
    "model = ppnlp.transformers.NeZhaForSequenceClassification.from_pretrained('nezha-large-wwm-chinese', num_classes=6)\n",
    "tokenizer = ppnlp.transformers.NeZhaTokenizer.from_pretrained('nezha-large-wwm-chinese')\n",
    "print(str.center(\"NeZha模型导入完毕\",80,\"=\"))\n",
    "\n",
    "# 导入模型权重参数\n",
    "params_path = '../checkpoint/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "    print(str.center(\"训练模型权重加载完毕\",80,\"=\"))\n",
    "\n",
    "# 模型预热\n",
    "batch_size = 1\n",
    "input_text = \"今天吃火锅，香死我了！！！\"\n",
    "predict(model, input_text, tokenizer, label_map, batch_size)\n",
    "print(str.center(\"模型预热完毕\",80,\"=\"))\n",
    "print(str.center(\"正在启动Web服务\",80,\"=\"))\n",
    "\n",
    "# 创建 FastAPI 实例\n",
    "PublicSentimentAnalysis = FastAPI()\n",
    "\n",
    "# 设置跨域\n",
    "PublicSentimentAnalysis.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=['*'],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# 单文本情感分析接口\n",
    "@PublicSentimentAnalysis.get(\"/singleSentimentAnalysis/\", status_code=200)\n",
    "# 定义路径操作函数，当接口被访问将调用该函数\n",
    "async def SingleSentimentAnalysis(text: str):\n",
    "    try:\n",
    "        # 获取用户输入的要进行属性级情感分析的文本内容\n",
    "        input_text = text\n",
    "        # 调用加载好的模型进行属性级情感分析\n",
    "        singleAnalysisResult = predict(model, input_text, tokenizer, label_map, batch_size)\n",
    "        # 接口结果返回\n",
    "        results = {\"message\": \"success\", \"inputText\": input_text, \"singleAnalysisResult\": singleAnalysisResult[0]}\n",
    "        return results\n",
    "    # 异常处理\n",
    "    except Exception as e:\n",
    "        print(\"异常信息：\", e)\n",
    "        raise HTTPException(status_code=500, detail=str(\"请求失败，服务器端发生异常！异常信息提示：\" + str(e)))\n",
    "\n",
    "# 建立后端服务\n",
    "# 本地部署服务运行后可以打开 http://localhost:8000/docs 进行接口调试\n",
    "uvicorn.run(PublicSentimentAnalysis, host=\"127.0.0.1\", port=8000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f17889-90ca-4861-ac97-9fb952e2a3a3",
   "metadata": {},
   "source": [
    "后端API接口测试图\n",
    "\n",
    "![后端接口调试图](https://ai-studio-static-online.cdn.bcebos.com/760f704d759b493eae1b2f388786f8e75dac988845d44137b531bb652af9c4d3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
